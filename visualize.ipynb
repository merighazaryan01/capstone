{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Xg3sAAW9EfOS"},"outputs":[],"source":["import os\n","import random\n","import matplotlib.pyplot as plt\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1741356295227,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"},"user_tz":-240},"id":"ikIUd2ptG1r2","outputId":"8b1967d0-783b-4288-820f-85321f243dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset directory exists: True\n"]}],"source":["import os\n","train_dir = \"/content/drive/MyDrive/capstone/dataset/train\"\n","print(\"Dataset directory exists:\", os.path.exists(train_dir))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1741356296038,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"},"user_tz":-240},"id":"WsIMLHc1KgqZ","outputId":"dd534f12-c12c-4deb-9b61-921c9a44ae4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["['.DS_Store', 'logoslogan_print', 'stripe', 'floral', 'disty_print', 'animal_print', 'dotted', 'allover', 'camouflage', 'color_block', 'plain', 'abstract', 'patchwork', 'graphic_print', 'checks', 'plain_denim']\n"]}],"source":["print(os.listdir(train_dir))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zff4jNmjG40o"},"outputs":[],"source":["def preview_dataset_images(train_dir, num_samples=2):\n","    \"\"\"\n","    Displays 'num_samples' images from each pattern class in the dataset.\n","\n","    Args:\n","        train_dir (str): Path to the training dataset directory.\n","        num_samples (int): Number of images to sample per pattern.\n","    \"\"\"\n","    if not os.path.exists(train_dir):\n","        print(f\"Dataset directory '{train_dir}' does not exist.\")\n","        return\n","\n","    # Get list of pattern folders\n","    pattern_classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n","\n","    fig, axes = plt.subplots(len(pattern_classes), num_samples, figsize=(num_samples * 3, len(pattern_classes) * 3))\n","\n","    for i, pattern in enumerate(pattern_classes):\n","        pattern_path = os.path.join(train_dir, pattern)\n","        image_files = [f for f in os.listdir(pattern_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n","\n","        if len(image_files) < num_samples:\n","            print(f\"Not enough images in '{pattern}'. Found {len(image_files)}, expected {num_samples}.\")\n","            sampled_images = image_files  # Use all available images\n","        else:\n","            sampled_images = random.sample(image_files, num_samples)\n","\n","        for j, img_name in enumerate(sampled_images):\n","            img_path = os.path.join(pattern_path, img_name)\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR (OpenCV) to RGB (Matplotlib)\n","\n","            if len(pattern_classes) == 1:\n","                ax = axes[j]\n","            else:\n","                ax = axes[i, j]\n","\n","            ax.imshow(img)\n","            ax.set_title(pattern)\n","            ax.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wJaB8Fh1thoHPFeEhMKLxezBblnGo-sB"},"executionInfo":{"elapsed":16962,"status":"ok","timestamp":1740498758656,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"},"user_tz":-240},"id":"1reWpZ0n-irW","outputId":"c06a3e69-dcef-47e7-819b-14de465a9fce"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["train_dir = \"/content/drive/MyDrive/capstone/dataset/train\"  # Update this to match your dataset path\n","preview_dataset_images(train_dir, num_samples=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3428,"status":"ok","timestamp":1740149915526,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"},"user_tz":-240},"id":"rXlEjED4-kt2","outputId":"f9496395-c0c0-4b57-d22c-1d06cf268d35"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n"]}],"source":["pip install pillow opencv-python\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ttI8UPYBTi9"},"outputs":[],"source":["import os\n","import random\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1987aCRTzXF9zQBHvjZP11TCpYduI279J"},"executionInfo":{"elapsed":4606,"status":"ok","timestamp":1740499279842,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"},"user_tz":-240},"id":"GJ2xmZ5EBKru","outputId":"6a95c2f8-6a7e-4c53-85c9-9ebade2be1d3"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["def display_images_colab(train_dir, num_samples=30):\n","    \"\"\"\n","    Displays 'num_samples' images per pattern inside a Colab notebook.\n","    \"\"\"\n","    if not os.path.exists(train_dir):\n","        print(f\"Dataset directory '{train_dir}' does not exist.\")\n","        return\n","\n","    pattern_classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n","\n","    for pattern in pattern_classes:\n","        pattern_path = os.path.join(train_dir, pattern)\n","        image_files = [f for f in os.listdir(pattern_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n","\n","        if len(image_files) < num_samples:\n","            print(f\"Not enough images in '{pattern}'. Found {len(image_files)}, expected {num_samples}.\")\n","            sampled_images = image_files\n","        else:\n","            sampled_images = random.sample(image_files, num_samples)\n","\n","        fig, axes = plt.subplots(5, 6, figsize=(15, 12))  # 5 rows × 6 columns (30 images max)\n","        fig.suptitle(f\"Pattern: {pattern}\", fontsize=14)\n","\n","        for ax, img_name in zip(axes.flatten(), sampled_images):\n","            img_path = os.path.join(pattern_path, img_name)\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n","\n","            ax.imshow(img)\n","            ax.set_title(img_name, fontsize=8)\n","            ax.axis(\"off\")\n","\n","        plt.show()\n","\n","# Usage\n","display_images_colab(train_dir, num_samples=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6LewkWGBtlV"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"SSgmtfPPt8VW"},"source":["get new dataset for only the patterns\n"]},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-e205XlXH9_q","executionInfo":{"status":"ok","timestamp":1741351440558,"user_tz":-240,"elapsed":120416,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"2c3ce35f-cc5f-4ba4-a185-0d0b417e8de2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-k4brzjt_\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-k4brzjt_\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from clip==1.0)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=837811f2888d7009719def622ecb4ac0d727e3639f81558f063b62947fb77af5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-gfa84c9k/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n","Successfully built clip\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","source":["import clip\n","import torch\n","from PIL import Image"],"metadata":{"id":"qThdZdy3IXj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_AIpPqtiIeA0","executionInfo":{"status":"ok","timestamp":1741352484391,"user_tz":-240,"elapsed":18262,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"71fb73ae-e9be-4357-a0ca-208135bd1fce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:09<00:00, 38.4MiB/s]\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xti5l4XbMDPm","executionInfo":{"status":"ok","timestamp":1740589002638,"user_tz":-240,"elapsed":13598,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"875e557c-4a39-4ff3-8029-8fd64e974a8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","\n","image_path = \"/content/drive/MyDrive/capstone/dataset/train/floral/8a72bf8ead09a9c2.jpg\"\n","\n","# Check if file exists\n","if os.path.exists(image_path):\n","    print(\"File found ✅\")\n","else:\n","    print(\"File NOT found ❌. Check the path!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEq2Ypy2JjoA","executionInfo":{"status":"ok","timestamp":1741352779871,"user_tz":-240,"elapsed":52,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"8aac4d43-82bf-4f89-e82e-9f1426a4de82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File found ✅\n"]}]},{"cell_type":"code","source":["image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n"],"metadata":{"id":"F-eceb7iIlq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = [\n","    \"abstract\", \"allover\", \"animal print\", \"camouflage\", \"checks\",\n","    \"color block\", \"disty print\", \"dotted\", \"floral\", \"graphic print\",\n","    \"logo slogan print\", \"patchwork\", \"plain\", \"plain denim\", \"stripe\"\n","]\n","\n","text_inputs = clip.tokenize(labels).to(device)\n"],"metadata":{"id":"v8X2iPqOMcrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text_inputs)\n","\n","    similarity = (image_features @ text_features.T).softmax(dim=-1)\n","    best_match = labels[similarity.argmax()]\n","    print(f\"Predicted Pattern: {best_match}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKJKJcnsMwr5","executionInfo":{"status":"ok","timestamp":1740589667607,"user_tz":-240,"elapsed":1405,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"c4a7602b-2dd7-407b-995e-7aa63eb7f185"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Pattern: graphic print\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","import clip\n","import torch\n","from PIL import Image\n","from tqdm import tqdm\n","\n","# Load CLIP model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","# Define dataset path\n","dataset_path = \"/content/drive/MyDrive/capstone/dataset/train/\"\n","\n","# Get all fabric pattern categories (folder names)\n","categories = sorted(os.listdir(dataset_path))\n","labels = [c.replace(\"_\", \" \") for c in categories]  # Convert folder names to text labels\n","text_inputs = clip.tokenize(labels).to(device)\n","\n","# Collect 50 random images from the dataset\n","image_samples = []\n","for category in categories:\n","    category_path = os.path.join(dataset_path, category)\n","    if os.path.isdir(category_path):\n","        images = [os.path.join(category_path, img) for img in os.listdir(category_path) if img.endswith((\".jpg\", \".png\"))]\n","        image_samples.extend(random.sample(images, min(4, len(images))))  # Take up to 4 per category\n","\n","# Shuffle and select only 50 images\n","random.shuffle(image_samples)\n","image_samples = image_samples[:50]\n","\n","# Track correct predictions\n","correct = 0\n","total = 0\n","\n","# Process the selected images\n","for img_path in tqdm(image_samples, desc=\"Processing images\"):\n","    category = os.path.basename(os.path.dirname(img_path))  # Get the actual category (ground truth)\n","\n","    try:\n","        # Load and preprocess image\n","        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n","\n","        # Predict with CLIP\n","        with torch.no_grad():\n","            image_features = model.encode_image(image)\n","            text_features = model.encode_text(text_inputs)\n","            similarity = (image_features @ text_features.T).softmax(dim=-1)\n","            predicted_label = labels[similarity.argmax()]\n","\n","        # Compare with ground truth\n","        if predicted_label == category.replace(\"_\", \" \"):\n","            correct += 1\n","        total += 1\n","\n","    except Exception as e:\n","        print(f\"Error processing {img_path}: {e}\")\n","\n","# Compute and display accuracy\n","accuracy = correct / total * 100 if total > 0 else 0\n","print(f\"\\nCLIP Accuracy on 50 Sampled Fabric Patterns: {accuracy:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z46OWPv2PIJe","executionInfo":{"status":"ok","timestamp":1740590277120,"user_tz":-240,"elapsed":100819,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"90d90714-f312-4c41-ac51-15a7f72f7808"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 50/50 [01:36<00:00,  1.93s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","CLIP Accuracy on 50 Sampled Fabric Patterns: 16.00%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","import clip\n","import torch\n","from PIL import Image\n","from tqdm import tqdm\n","\n","# Load CLIP model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","# Define test dataset path\n","test_dataset_path = \"/content/drive/MyDrive/capstone/dataset/test/\"\n","\n","# Get all fabric pattern categories (folder names)\n","categories = sorted(os.listdir(test_dataset_path))\n","labels = [c.replace(\"_\", \" \") for c in categories]  # Convert folder names to text labels\n","text_inputs = clip.tokenize(labels).to(device)\n","\n","# Collect 10 random images from the test dataset\n","image_samples = []\n","for category in categories:\n","    category_path = os.path.join(test_dataset_path, category)\n","    if os.path.isdir(category_path):\n","        images = [os.path.join(category_path, img) for img in os.listdir(category_path) if img.endswith((\".jpg\", \".png\"))]\n","        image_samples.extend(random.sample(images, min(2, len(images))))  # Take up to 2 per category\n","\n","# Shuffle and select only 10 images\n","random.shuffle(image_samples)\n","image_samples = image_samples[:10]\n","\n","# Process and predict each image\n","print(\"\\n--- CLIP Predictions for 10 Test Images ---\\n\")\n","for img_path in image_samples:\n","    category = os.path.basename(os.path.dirname(img_path))  # Get actual category (ground truth)\n","\n","    try:\n","        # Load and preprocess image\n","        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n","\n","        # Predict with CLIP\n","        with torch.no_grad():\n","            image_features = model.encode_image(image)\n","            text_features = model.encode_text(text_inputs)\n","            similarity = (image_features @ text_features.T).softmax(dim=-1)\n","            predicted_label = labels[similarity.argmax()]\n","\n","        # Print results\n","        print(f\"Image: {os.path.basename(img_path)}\")\n","        print(f\"    True Label: {category.replace('_', ' ')}\")\n","        print(f\"    CLIP Prediction: {predicted_label}\")\n","        print(\"-\" * 40)\n","\n","    except Exception as e:\n","        print(f\"Error processing {img_path}: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bwAZn_hQtig","executionInfo":{"status":"ok","timestamp":1740595823322,"user_tz":-240,"elapsed":25381,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"6bac0a6b-66e8-44eb-9668-3522b77a7075"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- CLIP Predictions for 10 Test Images ---\n","\n","Image: fec3838fd52ab401.jpg\n","    True Label: abstract\n","    CLIP Prediction: Graphic print\n","----------------------------------------\n","Image: bf6c397887c286c2.jpg\n","    True Label: abstract\n","    CLIP Prediction: Graphic print\n","----------------------------------------\n","Image: fe8190e7b1654978.jpg\n","    True Label: Camouflage\n","    CLIP Prediction: Graphic print\n","----------------------------------------\n","Image: eea8f0902d1b835f.jpg\n","    True Label: patchwork\n","    CLIP Prediction: patchwork\n","----------------------------------------\n","Image: aacac5859b64e897.jpg\n","    True Label: floral\n","    CLIP Prediction: Graphic print\n","----------------------------------------\n","Image: eae7839b96189691.jpg\n","    True Label: color block\n","    CLIP Prediction: Graphic print\n","----------------------------------------\n","Image: ea78934bf10dc1e8.jpg\n","    True Label: disty print\n","    CLIP Prediction: Graphic print\n","----------------------------------------\n","Image: ebb3b134c44cb5d0.jpg\n","    True Label: color block\n","    CLIP Prediction: Graphic print\n","----------------------------------------\n","Image: ed7aa2a8a08dd5d4.jpg\n","    True Label: plain\n","    CLIP Prediction: plain\n","----------------------------------------\n","Image: bea890df964aba41.jpg\n","    True Label: stripe\n","    CLIP Prediction: plain\n","----------------------------------------\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import clip\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import pickle  # To save embeddings\n","\n","# Load CLIP model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","# Define dataset path\n","train_dataset_path = \"/content/drive/MyDrive/capstone/dataset/train/\"\n","\n","# Get fabric pattern categories\n","categories = sorted(os.listdir(train_dataset_path))\n","category_to_idx = {category: idx for idx, category in enumerate(categories)}  # Mapping categories to numbers\n","\n","# Prepare lists to store features & labels\n","image_features_list = []\n","labels_list = []\n","\n","# Process each category\n","for category in tqdm(categories, desc=\"Processing categories\"):\n","    category_path = os.path.join(train_dataset_path, category)\n","    if not os.path.isdir(category_path):\n","        continue\n","\n","    for img_file in os.listdir(category_path):\n","        if img_file.endswith((\".jpg\", \".png\")):\n","            img_path = os.path.join(category_path, img_file)\n","\n","            try:\n","                # Load & preprocess image\n","                image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n","\n","                # Extract CLIP image features\n","                with torch.no_grad():\n","                    image_features = model.encode_image(image).cpu().numpy()\n","\n","                # Store features & label\n","                image_features_list.append(image_features.flatten())\n","                labels_list.append(category_to_idx[category])\n","\n","            except Exception as e:\n","                print(f\"Error processing {img_path}: {e}\")\n","\n","# Convert to numpy arrays\n","X_train = np.array(image_features_list)\n","y_train = np.array(labels_list)\n","\n","# Save embeddings for later use\n","with open(\"clip_train_embeddings.pkl\", \"wb\") as f:\n","    pickle.dump((X_train, y_train, category_to_idx), f)\n","\n","print(\"\\n✅ CLIP embeddings extracted and saved!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"y8SQp3_ZTAiN","executionInfo":{"status":"error","timestamp":1740591649763,"user_tz":-240,"elapsed":836768,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"e4c36388-0f28-43cc-e41b-92f56294cc04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing categories:  25%|██▌       | 4/16 [13:49<41:29, 207.44s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-7d63a2d2c344>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# Extract CLIP image features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Store features & label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mencode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NLD -> LND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# LND -> NLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import os\n","import torch\n","import clip\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import pickle\n","from torch.utils.data import DataLoader, Dataset\n","\n","# Load CLIP model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","# Define dataset path\n","train_dataset_path = \"/content/drive/MyDrive/capstone/dataset/train/\"\n","\n","# Get fabric pattern categories\n","categories = sorted(os.listdir(train_dataset_path))\n","category_to_idx = {category: idx for idx, category in enumerate(categories)}  # Mapping categories to numbers\n","\n","# Limit images per category (e.g., 50 per class)\n","MAX_IMAGES_PER_CATEGORY = 200\n","\n","# Custom dataset class for batch processing\n","class FabricDataset(Dataset):\n","    def __init__(self, image_paths, labels):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.image_paths[idx])\n","        image = preprocess(image)  # Preprocess using CLIP transform\n","        return image, self.labels[idx]\n","\n","# Collect image paths & labels\n","image_paths = []\n","labels = []\n","\n","for category in categories:\n","    category_path = os.path.join(train_dataset_path, category)\n","    if not os.path.isdir(category_path):\n","        continue\n","\n","    images = [os.path.join(category_path, img) for img in os.listdir(category_path) if img.endswith((\".jpg\", \".png\"))]\n","    sampled_images = images[:MAX_IMAGES_PER_CATEGORY]  # Limit per class\n","    image_paths.extend(sampled_images)\n","    labels.extend([category_to_idx[category]] * len(sampled_images))\n","\n","# Create dataset & dataloader\n","dataset = FabricDataset(image_paths, labels)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=False)  # Process 16 images at a time\n","\n","# Extract features in batches\n","image_features_list = []\n","labels_list = []\n","\n","print(\"\\nExtracting features...\")\n","\n","with torch.no_grad():\n","    for images, batch_labels in tqdm(dataloader, desc=\"Processing batches\"):\n","        images = images.to(device)\n","        batch_features = model.encode_image(images).cpu().numpy()\n","        image_features_list.extend(batch_features)\n","        labels_list.extend(batch_labels)\n","\n","# Convert to numpy arrays\n","X_train = np.array(image_features_list)\n","y_train = np.array(labels_list)\n","\n","# Save embeddings for later use\n","with open(\"clip_train_embeddings.pkl\", \"wb\") as f:\n","    pickle.dump((X_train, y_train, category_to_idx), f)\n","\n","print(\"\\n✅ CLIP embeddings extracted and saved!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hz0u6y4zV1Rf","executionInfo":{"status":"ok","timestamp":1740595313089,"user_tz":-240,"elapsed":27908,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"7248f206-cd26-4970-cdb5-0ab142287a2e"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Extracting features...\n"]},{"output_type":"stream","name":"stderr","text":["Processing batches:   3%|▎         | 6/188 [00:19<07:40,  2.53s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","Processing batches: 100%|██████████| 188/188 [14:21<00:00,  4.58s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ CLIP embeddings extracted and saved!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load saved embeddings\n","with open(\"clip_train_embeddings.pkl\", \"rb\") as f:\n","    X_train, y_train, category_to_idx = pickle.load(f)\n","\n","# Train an SVM classifier\n","svm = SVC(kernel=\"linear\", C=1.0)\n","svm.fit(X_train, y_train)\n","\n","# Save the trained classifier\n","with open(\"svm_classifier.pkl\", \"wb\") as f:\n","    pickle.dump((svm, category_to_idx), f)\n","\n","print(\"\\n✅ SVM classifier trained successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yO234ti-XcsG","executionInfo":{"status":"ok","timestamp":1740595319127,"user_tz":-240,"elapsed":623,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"82b13485-be4b-410e-aa40-c74cf019c1fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✅ SVM classifier trained successfully!\n"]}]},{"cell_type":"code","source":["# Define test dataset path\n","test_dataset_path = \"/content/drive/MyDrive/capstone/dataset/test/\"\n","\n","# Prepare lists to store test features & labels\n","test_features_list = []\n","test_labels_list = []\n","\n","# Process test images\n","for category in tqdm(categories, desc=\"Processing test categories\"):\n","    category_path = os.path.join(test_dataset_path, category)\n","    if not os.path.isdir(category_path):\n","        continue\n","\n","    for img_file in os.listdir(category_path):\n","        if img_file.endswith((\".jpg\", \".png\")):\n","            img_path = os.path.join(category_path, img_file)\n","\n","            try:\n","                # Load & preprocess image\n","                image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n","\n","                # Extract CLIP image features\n","                with torch.no_grad():\n","                    image_features = model.encode_image(image).cpu().numpy()\n","\n","                # Store features & label\n","                test_features_list.append(image_features.flatten())\n","                test_labels_list.append(category_to_idx[category])\n","\n","            except Exception as e:\n","                print(f\"Error processing {img_path}: {e}\")\n","\n","# Convert to numpy arrays\n","X_test = np.array(test_features_list)\n","y_test = np.array(test_labels_list)\n","\n","# Save test embeddings\n","with open(\"clip_test_embeddings.pkl\", \"wb\") as f:\n","    pickle.dump((X_test, y_test), f)\n","\n","print(\"\\n✅ Test CLIP embeddings extracted and saved!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oZM1smuXhDJ","executionInfo":{"status":"ok","timestamp":1740595728105,"user_tz":-240,"elapsed":387475,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"82d47286-b719-4331-d305-4c25bc18b669"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing test categories: 100%|██████████| 16/16 [06:27<00:00, 24.21s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Test CLIP embeddings extracted and saved!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Load trained classifier & test data\n","with open(\"svm_classifier.pkl\", \"rb\") as f:\n","    svm, category_to_idx = pickle.load(f)\n","\n","with open(\"clip_test_embeddings.pkl\", \"rb\") as f:\n","    X_test, y_test = pickle.load(f)\n","\n","# Predict on test data\n","y_pred = svm.predict(X_test)\n","\n","# Compute accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\n🔥 Final Model Accuracy on Test Data: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUagSipUZjor","executionInfo":{"status":"ok","timestamp":1740595732079,"user_tz":-240,"elapsed":366,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"dc0e7c10-a88b-46f4-c545-c241875ded7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🔥 Final Model Accuracy on Test Data: 75.77%\n"]}]},{"cell_type":"code","source":["with open(\"svm_classifier.pkl\", \"rb\") as f:\n","    svm, category_to_idx = pickle.load(f)\n","\n","# Reverse category mapping for readable output\n","idx_to_category = {idx: category for category, idx in category_to_idx.items()}\n","\n","# Collect 10 random images from the test set\n","image_samples = []\n","labels = []\n","\n","for category in category_to_idx.keys():\n","    category_path = os.path.join(test_dataset_path, category)\n","    if os.path.isdir(category_path):\n","        images = [os.path.join(category_path, img) for img in os.listdir(category_path) if img.endswith((\".jpg\", \".png\"))]\n","        sampled_images = random.sample(images, min(2, len(images)))  # Take 2 per category\n","        image_samples.extend(sampled_images)\n","        labels.extend([category] * len(sampled_images))\n","\n","# Shuffle and select only 10 images\n","random.shuffle(image_samples)\n","image_samples = image_samples[:100]\n","\n","# Process images and predict\n","print(\"\\n--- SVM Predictions on 10 Test Images ---\\n\")\n","for i, img_path in enumerate(image_samples):\n","    true_label = labels[i]\n","\n","    try:\n","        # Load & preprocess image\n","        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n","\n","        # Extract CLIP image features\n","        with torch.no_grad():\n","            image_features = model.encode_image(image).cpu().numpy().flatten()\n","\n","        # Predict using trained classifier\n","        predicted_idx = svm.predict([image_features])[0]\n","        predicted_label = idx_to_category[predicted_idx]\n","\n","        # Print results\n","        print(f\"🖼 Image: {os.path.basename(img_path)}\")\n","        print(f\"   ✅ True Label: {true_label.replace('_', ' ')}\")\n","        print(f\"   🔍 SVM Prediction: {predicted_label.replace('_', ' ')}\")\n","        print(\"-\" * 40)\n","\n","    except Exception as e:\n","        print(f\"Error processing {img_path}: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"clapgmXzgLA0","executionInfo":{"status":"error","timestamp":1740664997151,"user_tz":-240,"elapsed":700,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"52963b3c-cf6a-49a6-bbca-ca00274bd7ca"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'svm_classifier.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7ba478dcafad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"svm_classifier.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Reverse category mapping for readable output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0midx_to_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategory_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'svm_classifier.pkl'"]}]},{"cell_type":"markdown","source":["CNN"],"metadata":{"id":"IDD_1AUJtwMm"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchvision.utils import make_grid\n","\n"],"metadata":{"id":"h6jTPvtytwqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"metadata":{"id":"6vzZhfYAvy2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])\n"],"metadata":{"id":"SOaj1kw_v3lu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = \"/content/drive/MyDrive/capstone/dataset\"\n","train_dataset = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=transform)\n","test_dataset = datasets.ImageFolder(root=f\"{data_dir}/test\", transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"TGm9s6vPv4iI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n","        self.fc2 = nn.Linear(128, num_classes)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        x = x.view(x.size(0), -1)\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","num_classes = len(train_dataset.classes)\n","model = CNN(num_classes).to(device)"],"metadata":{"id":"1XcHoCg2wAYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"UOu3j90awDCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 2\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    total_batches = len(train_loader)\n","\n","    for batch_idx, (images, labels) in enumerate(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        if (batch_idx + 1) % 10 == 0 or batch_idx == total_batches - 1:\n","            print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{total_batches} | Loss: {loss.item():.4f}\")\n","\n","    print(f\"Epoch {epoch+1} completed. Average Loss: {running_loss/total_batches:.4f}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCUZuVMDwFrU","outputId":"de364f12-9961-490d-f7d7-35e0cae4a770"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/10 | Batch 10/709 | Loss: 2.6946\n","Epoch 1/10 | Batch 20/709 | Loss: 2.5912\n","Epoch 1/10 | Batch 30/709 | Loss: 2.3783\n","Epoch 1/10 | Batch 40/709 | Loss: 2.5769\n","Epoch 1/10 | Batch 50/709 | Loss: 2.6024\n","Epoch 1/10 | Batch 60/709 | Loss: 2.5964\n","Epoch 1/10 | Batch 70/709 | Loss: 2.5796\n","Epoch 1/10 | Batch 80/709 | Loss: 2.4128\n","Epoch 1/10 | Batch 90/709 | Loss: 2.5310\n","Epoch 1/10 | Batch 100/709 | Loss: 2.4280\n","Epoch 1/10 | Batch 110/709 | Loss: 2.5438\n","Epoch 1/10 | Batch 120/709 | Loss: 2.4911\n","Epoch 1/10 | Batch 130/709 | Loss: 2.6828\n","Epoch 1/10 | Batch 140/709 | Loss: 2.4495\n","Epoch 1/10 | Batch 150/709 | Loss: 2.4634\n","Epoch 1/10 | Batch 160/709 | Loss: 2.4131\n","Epoch 1/10 | Batch 170/709 | Loss: 2.4059\n","Epoch 1/10 | Batch 180/709 | Loss: 2.5457\n","Epoch 1/10 | Batch 190/709 | Loss: 2.5006\n","Epoch 1/10 | Batch 200/709 | Loss: 2.6120\n","Epoch 1/10 | Batch 210/709 | Loss: 2.4166\n","Epoch 1/10 | Batch 220/709 | Loss: 2.6702\n","Epoch 1/10 | Batch 230/709 | Loss: 2.4682\n","Epoch 1/10 | Batch 240/709 | Loss: 2.3374\n","Epoch 1/10 | Batch 250/709 | Loss: 2.3347\n","Epoch 1/10 | Batch 260/709 | Loss: 2.6384\n","Epoch 1/10 | Batch 270/709 | Loss: 2.4591\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/merighazaryan01/capstone.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w_pywqqH8r50","executionInfo":{"status":"ok","timestamp":1741725841533,"user_tz":-240,"elapsed":825,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"16998978-db6e-4d40-a17f-cfb00ec8e492"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'capstone'...\n","remote: Enumerating objects: 4, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (4/4), done.\n"]}]},{"cell_type":"code","source":["%cd capstone"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"658Do2Pd8-he","executionInfo":{"status":"ok","timestamp":1741725913281,"user_tz":-240,"elapsed":33,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"d3c6b02f-91d6-4ed3-a46e-c2c945fd452c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/capstone\n"]}]},{"cell_type":"code","source":["!ls \"/content/drive/My Drive\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6AX_xiZ9IG-","executionInfo":{"status":"ok","timestamp":1741726091737,"user_tz":-240,"elapsed":124,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"a384eb82-3433-45c0-bda2-057a98b45202"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":[" 9AF67E01-FD93-4F73-B3DD-C2D8B5B5B260.jpeg\n"," AI_Midterm1_Review.gdoc\n"," AI_Midterm2_Review.gdoc\n","'AI Project.gdoc'\n"," Attendance.gsheet\n"," Azerbaijan-Laundromat-Data-convertcsv-csv.xls\n","'Biology presentation.gdoc'\n","'Blank Quiz.gform'\n"," capstone\n"," chem_hw2\n"," chem_midterm2.ipynb\n","'Colab Notebooks'\n","'Copy of Blank Quiz.gform'\n","'Copy of Blank Quiz (Responses).gsheet'\n","\"Copy of CSE141 Fall'21 - Armvote21.gsheet\"\n","\"Copy of CSE141 Fall'21 - Makeover Party 1.gsheet\"\n","'Copy of Untitled spreadsheet.gsheet'\n"," DB_Homework2.gdoc\n"," Db_hw1_problem4.gsheet\n"," Introduction_to_Religion_Homework.gdoc\n","'Meri Ghazaryan'\n","'Meri Ghazaryan_Argumetative_Essay.docx'\n","'Meri Ghazaryan.gdoc'\n"," Meri_Ghazaryan_hw1.gdoc\n"," Meri_Ghazaryan_hw1.zip\n"," Meri_Ghazaryan_HW2.Ghazaryan\n"," Meri_Ghazaryan_Motivation_Letter.gdoc\n"," Meri_Ghazaryan_Project.ipynb\n","'Motivation Letter draft.gdoc'\n","'pollice2021 (1).pdf'\n"," pollice2021.gdoc\n"," pollice2021.pdf\n","'Python Backend Engineer (Intern).pdf'\n","'Recommendation Letter.gdoc'\n"," religion.gdoc\n","'Religion Midterm Questions.gdoc'\n","'The ability to change and seek opportunity has never been more valuable than it is today.gdoc'\n","'The Case of Saint Teresa .gdoc'\n"," Untitled0.ipynb\n"," Untitled1.ipynb\n"," Untitled2.ipynb\n","'Untitled document (10).gdoc'\n","'Untitled document (11).gdoc'\n","'Untitled document (12).gdoc'\n","'Untitled document (1).gdoc'\n","'Untitled document (2).gdoc'\n","'Untitled document (3).gdoc'\n","'Untitled document (4).gdoc'\n","'Untitled document (5).gdoc'\n","'Untitled document (6).gdoc'\n","'Untitled document (7).gdoc'\n","'Untitled document (8).gdoc'\n","'Untitled document (9).gdoc'\n","'Untitled document.gdoc'\n","'Untitled spreadsheet (1).gsheet'\n","'Untitled spreadsheet (2).gsheet'\n","'Untitled spreadsheet (3).gsheet'\n","'Untitled spreadsheet (4).gsheet'\n","'Untitled spreadsheet (5).gsheet'\n","'Untitled spreadsheet (6).gsheet'\n","'Untitled spreadsheet (7).gsheet'\n","'Untitled spreadsheet.gsheet'\n","'video-output-87304E06-1183-4982-B902-5464C585DDE5 (1).mov'\n"," video-output-87304E06-1183-4982-B902-5464C585DDE5.mov\n","'Լինա Շարազյան համեմատական վերլուծություն.docx'\n","'Տեքստի վերլուծություն.docx'\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvrenLhJ-Kc4","executionInfo":{"status":"ok","timestamp":1741726400156,"user_tz":-240,"elapsed":2208,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"0bd1f8cd-c3da-46eb-8b2b-45797f9ef276"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!mv /content/drive/My Drive/Colab Notebooks/visualize.ipynb /content/capstone/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgKO6nOU-dop","executionInfo":{"status":"ok","timestamp":1741726306943,"user_tz":-240,"elapsed":117,"user":{"displayName":"Meri Ghazaryan","userId":"01795401486263348555"}},"outputId":"5937250d-d157-419f-97b0-7badbe48e6af"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["mv: cannot stat '/content/drive/My': No such file or directory\n","mv: cannot stat 'Drive/Colab': No such file or directory\n","mv: cannot stat 'Notebooks/visualize.ipynb': No such file or directory\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1B_5qnPbbuA8ibzcewO2DpPclM80YRxbs","authorship_tag":"ABX9TyNXAjyJAjgOtbLKKkaKPmBX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}